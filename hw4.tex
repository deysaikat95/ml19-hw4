\documentclass[10pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{framed}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[letterpaper, margin=1in]{geometry}
\sloppy


\renewcommand{\familydefault}{ppl}

\newcommand{\bx}{{\boldsymbol x}}
\newcommand{\bX}{\mathbf {X}}
\newcommand{\bz}{{\boldsymbol z}}
\newcommand{\bh}{{\boldsymbol h}}
\newcommand{\bw}{{\boldsymbol w}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bmu}{{\boldsymbol \mu}}
\newcommand{\balpha}{{\boldsymbol \alpha}}
\newcommand{\bbeta}{{\boldsymbol \beta}}
\newcommand{\bSigma}{{\boldsymbol \Sigma}}
\newcommand{\btheta}{{\boldsymbol \theta}}
\newcommand{\reals}{{\mathbb R}}
\DeclareMathOperator*{\E}{{\mathbb E}}
\DeclareMathOperator*{\normal}{{\cal N}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\AND}{AND}
\DeclareMathOperator*{\OR}{OR}
\DeclareMathOperator*{\XOR}{XOR}
\DeclareMathOperator*{\sign}{sign}


\newcommand{\solution}[1]{{\color{PineGreen} \textbf{Solution:} #1}}

\newcommand{\todo}[1]{\textbf{\textcolor{MidnightBlue}{to do: #1}} }

\begin{document}

\section*{Machine Learning Homework 4}

\section*{General Instructions}

Homework must be submitted electronically on Canvas. Make sure to explain your reasoning or show your derivations. Except for answers that are especially straightforward, you will lose points for unjustified answers, even if they are correct. 

\section*{General Instructions}

You are allowed to work with at most one other student on the homework. With your partner, you will submit only one copy, and you will share the grade that your submission receives. You should set up your partnership on Canvas as a two-person group by joining one of the preset groups named ``HW4 Group $n$'' for some number $n$.

Submit your homework electronically on Canvas. We recommend using LaTeX, especially for the written problems. But you are welcome to use anything as long as it is neat and readable. 

For the programming portion, you should only need to modify the Python files. You may modify the iPython notebooks, but you will not be submitting them, so your code must work with our provided iPython notebook.

Relatedly, cite all outside sources of information and ideas. 

\section*{Written Problems}

\begin{enumerate}

\item We will derive the expectation-maximization (EM) algorithm using \textit{variational} analysis. 

Let $X = \{\bx_1, \bx_2, \ldots, \bx_n\}$ be a set of data vectors. Let $Z = \{z_1, \ldots, z_n\}$ be set of multinomial variables corresponding to which of $K$ Gaussians generated each example. Let the Gaussian parameters be means $\{\bmu_1, \ldots, \bmu_K\}$ and covariance matrices $\{\bSigma_1, \ldots, \bSigma_K\}$. Let $\Theta = \{\theta_1, \ldots, \theta_K\}$ be multinomial prior probabilities of which Gaussian generates each example.

Each data point is generated by first sampling a Gaussian index from $p(z | \Theta)$, then sampling from the Gaussian $\normal(\bmu_z, \bSigma_z)$. The log likelihood of any observations given these mixture model parameters is
\begin{align}
L(X, \bmu, \bSigma, \Theta) &= \sum_{i=1}^n \log \left(\sum_{k = 1}^K p(z_i | \Theta) \normal(x_i | \bmu_k, \Sigma_k) \right)
= \sum_{i=1}^n \log \left(\sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k) \right) ~.
\label{eq:likelihood}
\end{align}

Since we will never observe any $z$ variables, these are considered \textit{hidden} or \textit{latent} variables. When computing the likelihood of observed variables, we sum over all possible states of the latent variables, weighted by the probability of those states.

We start by doing something a little weird. We create an independent distribution $q$ for the latent variables such that
\begin{align}
q(z_1, \ldots, z_n) &:= q(z_1) q(z_2) \ldots q(z_n).
\end{align}
We then rewrite the log likelihood from \cref{eq:likelihood} so that each data point's likelihood is multiplied by the $q$ distribution divided by itself. In other words, we multiply the terms in the innermost summation by $\frac{q(z_i = k)}{q(z_i = k)}$, resulting in the equivalent form of the likelihood:
\begin{align}
L(X, \bmu, \bSigma, \Theta, q) =& \sum_{i=1}^n \log \left(\sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k) \underbrace{q(z_i = k) / q(z_i = k)}_{1} \right) .
\end{align}

Jensen's inequality guarantees that for any convex function $\varphi$ and any distribution over random variable $X$,
\begin{align}
\varphi\left( \E \left[ X \right] \right) \le \E \left[  \varphi(X) \right]~.
\label{eq:jensen}
\end{align}
We use Jensen's inequality and the fact that $\log$ is a \textbf{concave} function (i.e., $-\log$ is a convex function)  to form a lower bound on the log likelihood:
\begin{align}
L(X, \bmu, \bSigma, \Theta, q) =& \sum_{i=1}^n \log \left(\sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k) q(z_i = k) / q(z_i = k) \right)\\
& \ge \sum_{i=1}^n \sum_{k = 1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k) / q(z_i = k) \right)\\
& = \sum_{i=1}^n \sum_{k = 1}^K q(z_i = k) \log\left(\theta_k \normal(x_i | \bmu_k, \bSigma_k)\right) - \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) \log q(z_i = k) .
\end{align}

\begin{enumerate}


\item (5 points) You now have a lower bound objective function that depends on the Gaussian mixture model parameters $\bmu$, $\bSigma$, and $\Theta$ and the variational distribution $q$. If we hold $q$ fixed, maximizing the Gaussian mixture model parameters gets us the Gaussian EM updates (the so-called ``m-step''). You will prove this fact for one of the mixture parameters. (You are welcome to try the other parameters too for fun.) Show that maximizing the lower bound with respect to the cluster mixture probabilities $\Theta$ is exactly the EM update for these variables. 

You will need to use a Lagrange multiplier to enforce that $1 = \sum_{k=1}^K \theta_k$. Then solve for the settings of the Lagrange multiplier and each parameter $\theta_k$ to find the optimum of the constrained optimization. In other words, if the Lagrange multiplier is $\zeta$ and the Lagrangian form of the objective function is $\tilde{L}$, then you can find the solution by solving the following equations:
\begin{equation}
\begin{aligned}
\frac{\partial ~ \tilde{L}}{\partial ~ \theta_k} = 0, ~~~~~\textrm{and}~~~~~ \frac{\partial ~ \tilde{L}}{\partial ~ \zeta} = 0.
\end{aligned}
\end{equation}
Solve each equation in order and plug in the result into the original Lagrangian objective. Each zero-derivative condition will tell you something about the original objective that allows you to simplify it. 
You should end up with a final formula for the optimal value of $\theta_k$ that is relatively compact; Terms should simplify significantly to result in a simple final expression.


\item (5 points) This second part is a bit more involved, but follows a similar line of reasoning. Show how to find the $q$ parameters for each data point that maximize the lower bound. 

You'll need to use Lagrange multipliers to enforce that $1 = \sum_k q(z_i = k)$ for each $i$, and you should be able to consider each data point's $q$ distribution independently.
Then solve for the settings of the Lagrange multiplier and each parameter $q(z_i = k)$ to find the optimum of the constrained optimization. In other words, if the Lagrange multiplier for the $i$'th variable is $\zeta_i$ and the Lagrangian form of the objective function is $\tilde{L}$, then you can find the solution by solving the following equations:
\begin{equation}
\begin{aligned}
\frac{\partial ~ \tilde{L}}{\partial ~ q(z_i = k)} = 0, ~~~~~\textrm{and}~~~~~ \frac{\partial ~ \tilde{L}}{\partial ~ \zeta_i} = 0,
\end{aligned}
\end{equation}
where we abuse notation to refer to the multinomial probability $q(z_i = k)$ as a variable.

You should again end up with a final formula for the optimal value of $q(z_i = k)$ that is relatively compact; Terms should simplify significantly to result in a simple final expression. 



\end{enumerate}

\item (5 points) Project proposal. See instructions on project homepage.

\end{enumerate}

\section*{Programming Assignment}

For this programming assignment, we have provided a lot of starter code. Your tasks will be to complete the code in a few specific places, which will require you to read and understand most of the provided code but will only require you to write a small amount of code yourself.

We have provided synthetic data that was generated with the following procedure: 

\begin{itemize}

\item First, we generate a two-dimensional random Gaussian mixture model with five Gaussians.

\item We then generate $n$ (2000) two-dimensional data points from the mixture model.

\item We project the two-dimensional data into 64-dimensional data using a random projection matrix.

\item Finally, we add random noise in 64-dimensional space to the projected data.

\end{itemize}

The resulting data comes from a two-dimensional mixture model, but it is presented using 64-dimensions. You will use PCA and expectation maximization to attempt to fit a mixture model to the data.

The main experiment notebook is \texttt{run\_synthetic\_experiment.ipynb}. It first uses PCA to transform the data into the two most descriptive dimensions. It then runs k-means clustering on the data as a demonstration (k-means is already implemented for you directly in the notebook, but it probably won't work well unless your PCA implementation is correct). Then it runs expectation maximization using different numbers of Gaussians and records the log-likelihood of held-out validation data for each cluster count parameter. 

A second experiment notebook called \texttt{eigen\_and\_gmm\_faces.ipynb} runs a similar experiment using image data of faces. It uses PCA to reduce the dimensionality of raw-pixel representations of grayscale images. Then it uses a Gaussian mixture model to fit the shape of the reduced-dimension data so that it can generate random faces that look somewhat realistic.

We included a unit test class \texttt{test\_pca\_gmm.py} that runs PCA and Gaussian mixture modeling and runs a few tests. Your implementation should pass these tests in almost all cases. There is some randomness involved in the mixture modeling, so you may have very rare cases where a bad initialization causes a test to fail. But if you consistently fail the tests, then your implementation is probably incorrect.

Since various Python libraries include the functionality to do PCA and Gaussian mixture fitting, you are not allowed to submit code that uses these pre-built functions. You are however allowed to use the built-in numerical linear algebra functions \texttt{numpy.linalg.det}, \texttt{numpy.linalg.eig}, \texttt{numpy.linalg.svd}, or \texttt{numpy.linalg.cholesky}. (You don't need all of these, but you may want to use one or two.) You may also use any of the helper functions in \texttt{gmm.py}, especially \texttt{gaussian\_ll}, which computes the log of the Gaussian likelihood for a set of data given a mean and covariance matrix.

\begin{enumerate}

\item (5 points) Complete the function \texttt{pca} in \texttt{pca.py}. The function should take a data matrix as input and return a transformed data matrix and the variance of the transformed data. 
The dimensions of the transformed data should be ordered such that the earlier indices have the highest variance. Using this ordering, one should be able to truncate to a lower-dimensional space while preserving the highest reconstruction accuracy by using the first few dimensions of the returned data.

\item (6 points) Complete \texttt{gmm} and \texttt{compute\_membership} in \texttt{gmm.py}. The \texttt{gmm} function should take a data matrix and a number of clusters as input and fit the Gaussians using expectation maximization. 
And the \texttt{compute\_membership} function should compute the likelihood for each example that it came from each of the $K$ Gaussians, returning the likelihoods in a matrix with $K$ rows and $n$ columns. 
The main framework for the algorithm is set up for you already. You need to complete the code for updating the parameters (the Gaussian means and covariances and the cluster priors) and the latent variable probabilities (the cluster assignments).

One important note for fitting Gaussians is that sometimes we can get degenerate data that creates numerically unstable covariances. To avoid this problem, one fix is to add a small constant to the diagonal of the estimated covariance matrix. The matrix \texttt{reg} in \texttt{gmm} is set up for you to do this. You can argue that this trick is regularization, or if you're more honest, it's just a reasonable hack to make things work.

\item (4 points) Complete the function \texttt{gmm\_ll} in \texttt{gmm.py}. This function should take a data matrix and Gaussian mixture model parameters as input and output a log likelihood. The formula for the log likelihood is
\begin{equation}
\begin{aligned}
\mathrm{ll}(\bX, \bmu, \bSigma, \btheta) &= \log \prod_{i=1}^n p(\bx_i | \bmu, \bSigma, \btheta)\\
&= \log \prod_{i=1}^n \sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k)\\
&=  \sum_{i=1}^n \log \left(\sum_{k = 1}^K \theta_k \normal(x_i | \bmu_k, \Sigma_k) \right)\\
&=  \sum_{i=1}^n \log \left(\sum_{k = 1}^K  \frac{\theta_k}{\sqrt{(2\pi)^d|\Sigma_k|}} \exp\left(- \frac{1}{2} (\bx_i - \bmu_k)^\top \Sigma_k^{-1} (\bx_i - \bmu_k)\right) \right).
\end{aligned}
\end{equation}
With some manipulation, you should be able to use the log-sum-exp trick to compute the terms in the summation while avoiding numerical underflow. Notice that the term inside the innermost summation can be written as the $\exp$ of the log density of a Gaussian distribution. The log Gaussian density is returned by the provided function \texttt{gaussian\_ll}.
We provided an implementation of \texttt{logsumexp} at the bottom of \texttt{gmm.py}.

\end{enumerate}

Once you complete these three pieces, you should be able to run the entirety of both experiment scripts. You should see some structure in the synthetic data points if PCA works correctly to identify the non-noise dimensions of the data, and you should see how using more Gaussians leads to higher likelihood with diminishing returns. And on the face-data experiment, you should see some plausible faces sampled from the learned Gaussian mixture model. (They are not very realistic compared to more modern deep generative models.)

\end{document}


